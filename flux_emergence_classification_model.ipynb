{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flux_emergence_classification_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGSem6HRzMgv"
      },
      "source": [
        "# **Importing appropriate packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsRvsjTy-IQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8992bfbf-cfd1-4914-aaf7-a7675cd9d585"
      },
      "source": [
        "%tensorflow_version 1.X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.X`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzC6A11dQ_Bs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "41e2dd48-9245-42ac-9ec0-e6710e5716a6"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "print(tensorflow.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srvzcAwIQmpD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "4fa6f15c-b060-46ed-b9b4-2151d7862466"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import backend as K\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import pickle\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "\n",
        "seed_value = 5421\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "startDir = '/content/drive/My Drive/Colab Notebooks/swri_research/data/'\n",
        "weights_dir = startDir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNJeqB6hx_BK"
      },
      "source": [
        "# **Designing Data Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3cvrPEA-WB5"
      },
      "source": [
        "from scipy.interpolate import interp1d\n",
        "from skimage.measure import label, regionprops\n",
        "import csv\n",
        "def imageLoaderNew(files, batch_size):\n",
        "  L = len(files)\n",
        "  k = 0\n",
        "  num = np.int(L/batch_size) \n",
        "  #this line is just to make the generator infinite, keras needs that    \n",
        "  while True:\n",
        "    k = k % num\n",
        "    f_batch = files[batch_size*k:batch_size*(k+1)]\n",
        "    X = np.zeros((batch_size,90,90,225),dtype=float)\n",
        "    X_b=np.zeros((90,90,225*batch_size))\n",
        "    for r in range(batch_size):\n",
        "      y = fits.open(f_batch[r])\n",
        "      yy = fits.open(f_batch[r][:-5]+'.dt.fits')\n",
        "      imgs = np.transpose(y[0].data)\n",
        "      imgs=np.clip(imgs,-1000,1000)\n",
        "      imgs=0.5*(1+imgs/1000)\n",
        "      frame=yy[0].data/5760\n",
        "      frame=frame.astype('int')\n",
        "      mx=int(np.max(frame)+1)\n",
        "      if r==0:\n",
        "        ind_q=frame\n",
        "        ind_i=np.arange(mx)\n",
        "      else:\n",
        "        ind_q=np.append(ind_q,225*r+frame)\n",
        "        ind_i=np.append(ind_i,225*r+np.arange(mx))\n",
        "\n",
        "      for i in range(len(frame)):\n",
        "        X[r,:,:,frame[i]] = imgs[:,:,i]\n",
        "      for i in range(mx,225):\n",
        "        X[r,:,:,i] = imgs[:,:,len(frame)-1]\n",
        "      \n",
        "      X_b[:,:,r*225:(r+1)*225]=X[r,:,:,:]\n",
        "    \n",
        "    for i in range(90):\n",
        "      for j in range(90):\n",
        "        f_q = X_b[i,j,ind_q]\n",
        "        f = interp1d(ind_q, f_q)\n",
        "        X_b[i,j,ind_i]=f(ind_i)\n",
        "   \n",
        "    for l in range(batch_size):\n",
        "      X[l,:,:,:]=X_b[:,:,l*225:(l+1)*225]\n",
        "\n",
        "    im = (X==0)\n",
        "    for i in range(batch_size):\n",
        "      for j in range(225):\n",
        "        label_image = label(im[i,:,:,j])\n",
        "        mn=np.min(label_image)\n",
        "        mx=np.max(label_image)+1\n",
        "        for r in range(mn,mx):\n",
        "          bw=(label_image == r)*im[i,:,:,j]\n",
        "          if bw[0,0]==1 or bw[0,89]==1 or bw[89,0]==1 or bw[89,89]==1:\n",
        "            X[i,:,:,j]=0.5*bw+X[i,:,:,j]\n",
        "\n",
        "    Y = np.zeros(batch_size)\n",
        "    for i in range(batch_size):\n",
        "      fl = open('/content/drive/My Drive/Colab Notebooks/swri_research/data/BARD_v2/BARD_v2_data_file_path_labels_pass_f.csv', 'r')\n",
        "      reader = csv.reader(fl)\n",
        "      for row in reader:\n",
        "        if row[35] == f_batch[i][58:]:\n",
        "          Y[i] = int(row[36])\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "    yield(X,Y)\n",
        "    yield (1-X,Y) \n",
        "    yield(np.flip(X,2),Y)\n",
        "    yield(1-np.flip(X,2),Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOUTYG7EylX6"
      },
      "source": [
        "# **Creating Training, Validation combinations for Model Ensemble**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E2i4u_yPEWP"
      },
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from astropy.io import fits\n",
        "import glob,os\n",
        "import pickle\n",
        "\n",
        "s=(800,90,90,200)\n",
        "data_learn=np.zeros(s)\n",
        "\n",
        "f_test_y = []\n",
        "f_test_n = []\n",
        "\n",
        "f_name_y=[]\n",
        "dir_path_y='/content/drive/My Drive/Colab Notebooks/swri_research/data/BARD_v2/Emergence/'\n",
        "for root, dirs, files in os.walk(dir_path_y+'Training/'): \n",
        "    for file in files:  \n",
        "        if file.endswith('dt.fits'):\n",
        "          fh = root+file\n",
        "          fd = fh[:-8]+'.fits'\n",
        "          f = pathlib.Path(fd)\n",
        "          if f.exists() and int(fd[-9:-7])<11:\n",
        "            f_name_y.append(fd)\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(dir_path_y+'Validation/'): \n",
        "    for file in files:  \n",
        "        if file.endswith('dt.fits'):\n",
        "          fh = root+file\n",
        "          fd = fh[:-8]+'.fits'\n",
        "          f = pathlib.Path(fd)\n",
        "          if f.exists() and int(fd[-9:-7])<11:\n",
        "            f_name_y.append(fd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(dir_path_y+'Test/'): \n",
        "    for file in files:  \n",
        "        if file.endswith('dt.fits'):\n",
        "          fh = root+file\n",
        "          fd = fh[:-8]+'.fits'\n",
        "          f = pathlib.Path(fd)\n",
        "          if f.exists():\n",
        "            if f.exists() and fd[-9:-7]=='10':\n",
        "              f_name_y.append(fd)\n",
        "            if fd[-9:-7]=='11' or fd[-9:-7]=='12':\n",
        "              f_test_y.append(fd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f_name_n=[]\n",
        "dir_path_n='/content/drive/My Drive/Colab Notebooks/swri_research/data/BARD_v2/AR_Non_Emergence/'\n",
        "for root, dirs, files in os.walk(dir_path_n+'Training/'): \n",
        "    for file in files:  \n",
        "        if file.endswith('dt.fits'):\n",
        "          fh = root+file\n",
        "          fd = fh[:-8]+'.fits'\n",
        "          f = pathlib.Path(fd)\n",
        "          if f.exists() and int(fd[-9:-7])<11:\n",
        "            f_name_n.append(fd)\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(dir_path_n+'Validation/'): \n",
        "    for file in files:  \n",
        "        if file.endswith('dt.fits'):\n",
        "          fh = root+file\n",
        "          fd = fh[:-8]+'.fits'\n",
        "          f = pathlib.Path(fd)\n",
        "          if f.exists() and int(fd[-9:-7])<11:\n",
        "            f_name_n.append(fd)\n",
        "\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(dir_path_n+'Test/'): \n",
        "    for file in files:  \n",
        "        if file.endswith('dt.fits'):\n",
        "          fh = root+file\n",
        "          fd = fh[:-8]+'.fits'\n",
        "          f = pathlib.Path(fd)\n",
        "          if f.exists():\n",
        "            if f.exists() and fd[-9:-7]=='10':\n",
        "              f_name_n.append(fd)\n",
        "            if fd[-9:-7]=='11' or fd[-9:-7]=='12':\n",
        "              f_test_n.append(fd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Oovoll3xnt"
      },
      "source": [
        "f_yn=[]\n",
        "f_yn_test = []\n",
        "for i in range(len(f_name_y)):\n",
        "  f_yn.append(f_name_y[i])\n",
        "\n",
        "for i in range(len(f_name_n)):\n",
        "  f_yn.append(f_name_n[i])\n",
        "\n",
        "for i in range(len(f_test_y)):\n",
        "  f_yn_test.append(f_test_y[i])\n",
        "\n",
        "for i in range(len(f_test_n)):\n",
        "  f_yn_test.append(f_test_n[i])\n",
        "\n",
        "\n",
        "num=[]\n",
        "for i in range(len(f_yn)):\n",
        "    num.append(f_yn[i][-13:-5])\n",
        "\n",
        "num_test=[]\n",
        "for i in range(len(f_yn_test)):\n",
        "    num_test.append(f_yn_test[i][-13:-5])\n",
        "\n",
        "arg=np.argsort(num)\n",
        "\n",
        "arg_test = np.argsort(num_test)\n",
        "\n",
        "f_final=[]\n",
        "\n",
        "for i in range(len(arg)):\n",
        "    f_final.append(f_yn[arg[i]])\n",
        "\n",
        "f_final_test = []\n",
        "\n",
        "for i in range(len(arg_test)):\n",
        "    f_final_test.append(f_yn_test[arg_test[i]])\n",
        "\n",
        "val_months=['05','06']\n",
        "files_train=[]\n",
        "files_val=[]\n",
        "for i in range(len(f_final)):\n",
        "  if f_final[i][-9:-7]==val_months[0] or f_final[i][-9:-7]==val_months[1]:\n",
        "    files_val.append(f_final[i])\n",
        "  else:\n",
        "    files_train.append(f_final[i])\n",
        "\n",
        "\n",
        "print(files_val)\n",
        "print(len(files_val))\n",
        "print(files_train)\n",
        "print(len(files_train))\n",
        "print(len(f_final))\n",
        "print(f_final_test)\n",
        "print(len(f_final_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBFFQuQnPxUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "64903a6d-3fbd-4606-a15d-24b0f654c54e"
      },
      "source": [
        "fileList_train = files_train[:int(10*np.floor(0.1*len(files_train)))]\n",
        "print(len(fileList_train))\n",
        "fileList_valid = files_val[:int(10*np.floor(0.1*len(files_val)))]\n",
        "print(len(fileList_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1580\n",
            "440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok07KhnzyvRD"
      },
      "source": [
        "# **Designing the CNN and Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K6GWkJk6O3G"
      },
      "source": [
        "from keras.backend import sigmoid\n",
        "def swish(x, beta = 1):\n",
        "    return (x * sigmoid(beta * x))\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "get_custom_objects().update({'swish': Activation(swish)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBVn0wrIY7Fl"
      },
      "source": [
        "# #####Model with BatchNorm#####\n",
        "# #(x,y,ch)\n",
        "# #(90,90,225)\n",
        "model = Sequential() \n",
        "model.add(Conv2D(32, (3, 3), input_shape = (90,90,225),padding='same')) \n",
        "model.add(Activation('swish'))\n",
        "model.add(BatchNormalization()) \n",
        "model.add(Conv2D(32, (3, 3),padding='same')) \n",
        "model.add(Activation('swish')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n",
        "\n",
        "#(45,45,32)\n",
        "model.add(Conv2D(64, (3, 3))) \n",
        "model.add(Activation('swish'))\n",
        "model.add(BatchNormalization()) \n",
        "model.add(Conv2D(64, (3, 3))) \n",
        "model.add(Activation('swish')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n",
        "\n",
        "#(20,20,64)\n",
        "model.add(Conv2D(128, (3, 3))) \n",
        "model.add(Activation('swish'))\n",
        "model.add(BatchNormalization()) \n",
        "model.add(Conv2D(128, (3, 3))) \n",
        "model.add(Activation('swish')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n",
        "\n",
        "\n",
        "\n",
        "#(8,8,128)\n",
        "model.add(Conv2D(256, (3, 3))) \n",
        "model.add(Activation('swish'))\n",
        "model.add(BatchNormalization()) \n",
        "model.add(Conv2D(256, (3, 3))) \n",
        "model.add(Activation('swish')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n",
        "\n",
        "\n",
        "#(2,2,256)\n",
        "model.add(Conv2D(512, (2, 2))) \n",
        "model.add(Activation('swish'))\n",
        "\n",
        "\n",
        "#(1,1,512)  \n",
        "model.add(Flatten()) \n",
        "model.add(Dense(512)) \n",
        "model.add(Activation('swish')) \n",
        "model.add(Dropout(0.5,seed=seed_value)) \n",
        "model.add(Dense(512)) \n",
        "model.add(Activation('swish')) \n",
        "model.add(Dropout(0.5,seed=seed_value))\n",
        "model.add(Dense(1)) \n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2DgMDNc-K_9"
      },
      "source": [
        "sgd = SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "model.compile(loss ='binary_crossentropy',  optimizer = sgd, metrics =['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDCN4Jwreqok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d6b2b79-e65a-41b5-ce45-5d495793dd49"
      },
      "source": [
        "batch_size=10\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_56_pass_f_v4.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1, mode='auto')\n",
        "history=model.fit(imageLoaderNew(fileList_train, batch_size), steps_per_epoch = 4*len(fileList_train)/batch_size, epochs = 20,callbacks=[checkpoint,early_stopping], validation_data=imageLoaderNew(fileList_valid, batch_size), validation_steps = 4*len(fileList_valid)/batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/20\n",
            "  4/632 [..............................] - ETA: 48:31 - loss: 0.7941 - accuracy: 0.4750  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:718: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "632/632 [==============================] - 2428s 4s/step - loss: 0.6342 - accuracy: 0.6456 - val_loss: 0.6116 - val_accuracy: 0.6477\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.64773, saving model to /content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_56_pass_f_v4.h5\n",
            "Epoch 2/20\n",
            "632/632 [==============================] - 1102s 2s/step - loss: 0.5217 - accuracy: 0.7150 - val_loss: 0.5332 - val_accuracy: 0.6960\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.64773 to 0.69602, saving model to /content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_56_pass_f_v4.h5\n",
            "Epoch 3/20\n",
            "632/632 [==============================] - 1109s 2s/step - loss: 0.4408 - accuracy: 0.7826 - val_loss: 0.3647 - val_accuracy: 0.7426\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.69602 to 0.74261, saving model to /content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_56_pass_f_v4.h5\n",
            "Epoch 4/20\n",
            "632/632 [==============================] - 1115s 2s/step - loss: 0.3870 - accuracy: 0.8098 - val_loss: 0.7074 - val_accuracy: 0.6955\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.74261\n",
            "Epoch 5/20\n",
            " 58/632 [=>............................] - ETA: 12:20 - loss: 0.3913 - accuracy: 0.8224"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTRYHi_SzAIy"
      },
      "source": [
        "# **Estimating Uncertainty of Emergence Probability on Test Set using Model Ensemble**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIEue_6BfhGK"
      },
      "source": [
        "###for uncertainty of emergence probability##\n",
        "from scipy.interpolate import interp1d\n",
        "from skimage.measure import label, regionprops\n",
        "from astropy.io import fits\n",
        "import csv\n",
        "new_row=[]\n",
        "with open('/content/drive/My Drive/Colab Notebooks/swri_research/data/BARD_v2/BARD_v2_file_test_path_label_f.csv', 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    if row[0]!='Path':\n",
        "      path = '/content/drive/My Drive/Colab Notebooks/swri_research/data' + row[0] \n",
        "      y = fits.open(path)\n",
        "      yy = fits.open(path[:-5]+'.dt.fits')\n",
        "      imgs = np.transpose(y[0].data)\n",
        "      imgs=np.clip(imgs,-1000,1000)\n",
        "      imgs=0.5*(1+imgs/1000)\n",
        "      frame=yy[0].data/5760\n",
        "      frame=frame.astype('int')\n",
        "      lst = frame.tolist()\n",
        "      miss = []\n",
        "      mn=int(np.min(frame))\n",
        "      mx=int(np.max(frame)+1)\n",
        "      for i in range(mn,mx):\n",
        "        if i not in lst:\n",
        "          miss.append(i)\n",
        "\n",
        "      X = np.zeros((1,90,90,225),dtype=float)\n",
        "      for i in range(len(frame)):\n",
        "        X[0,:,:,frame[i]] = imgs[:,:,i]\n",
        "      for i in range(mx,225):\n",
        "        X[0,:,:,i] = imgs[:,:,len(frame)-1]\n",
        "      ind_q=frame\n",
        "      ind_i=np.arange(mx)\n",
        "      for i in range(90):\n",
        "        for j in range(90):\n",
        "          f_q = X[0,i,j,ind_q]\n",
        "          f = interp1d(ind_q, f_q, kind = 'linear')\n",
        "          X[0,i,j,ind_i]=f(ind_i)\n",
        "\n",
        "      im = (X==0)\n",
        "      for j in range(225):\n",
        "        label_image = label(im[0,:,:,j])\n",
        "        mn=np.min(label_image)\n",
        "        mx=np.max(label_image)+1\n",
        "        for r in range(mn,mx):\n",
        "          bw=(label_image == r)*im[0,:,:,j]\n",
        "          if bw[0,0]==1 or bw[0,89]==1 or bw[89,0]==1 or bw[89,89]==1:\n",
        "            X[0,:,:,j]=0.5*bw+X[0,:,:,j]\n",
        "\n",
        "    \n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_12_pass_f_v3.h5')\n",
        "      row[5] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_34_pass_f_v3.h5')\n",
        "      row[6] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_56_pass_f_v3.h5')\n",
        "      row[7] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_78_pass_f_v3.h5')  \n",
        "      row[8] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_9X_pass_f_v3.h5')  \n",
        "      row[9] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_12_pass_f_v4.h5')\n",
        "      row[10] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_34_pass_f_v4.h5')\n",
        "      row[11] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_56_pass_f_v4.h5')\n",
        "      row[12] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_78_pass_f_v4.h5')  \n",
        "      row[13] = model.predict(X)[0][0]\n",
        "      model.load_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emrg_weights_newData_vgg_9X_pass_f_v4.h5')  \n",
        "      row[14] = model.predict(X)[0][0]\n",
        "      print(row)\n",
        "      new_row.append(row)\n",
        "    else:\n",
        "      new_row.append(row)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/swri_research/data/BARD_v2/BARD_v2_file_test_path_label_f.csv', 'w') as f:\n",
        "    # Overwrite the old file with the modified rows\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(new_row)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}